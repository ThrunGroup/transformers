# model names
TRANSFORMER_XL = "transformer_xl"
GPT2 = "gpt2"
GPT2_MEDIUM = "gpt2-medium"
GPT2_LARGE = "gpt2-large"
GPT2_XL = "gpt2-xl"
OPT_1_3B = "opt-1.3b"
OPT_350M = "opt-350m"
OPT_125M = "opt-125m"
OPT = "opt"

# accelerators
SVD = "SVD"
PCA = "PCA"
QUANTIZATION = "QUANTIZATION"

# Quantization types
DynamicQ = "Dynamic Quantization"
StaticQ = "Static Quantization"
QAT = "Quantization Aware Training"

# Quantization example input
EXAMPLE_INPUT = "Hello, my name is"

# datasets
BILLSUM = "billsum"
WIKITEXT2 = "wikitext-2-raw-v1"

# Number of blocks
NUM_BLOCKS_GPT2 = 12
