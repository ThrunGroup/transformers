# model names
TRANSFORMER_XL = "transformer_xl"
GPT2 = "gpt2"
GPT2_MEDIUM = "gpt2-medium"
GPT2_LARGE = "gpt2-large"
GPT2_XL = "gpt2-xl"
OPT = "opt"
OPT_125M = "opt-125m"
OPT_350M = "opt-350m"
OPT_1_3B = "opt-1.3b"
OPT_2_7B = "opt-2.7b"
OPT_13B = "opt-13b"
OPT_30B = "opt-30b"

BLOOM = "bloom"
BLOOM_560M = "bloom-560m"
BLOOM_1b = "bloom-1b1"
BLOOM_3b = "bloom_3b"
BLOOM_7b1 = "bloom_7b1"


# accelerators
SVD = "SVD"
PCA = "PCA"
QUANTIZATION = "QUANTIZATION"

# Quantization types
DynamicQ = "Dynamic Quantization"
StaticQ = "Static Quantization"
QAT = "Quantization Aware Training"

# Quantization example input
EXAMPLE_INPUT = "Hello, my name is"

# datasets
BILLSUM = "billsum"
WIKITEXT2 = "wikitext-2-raw-v1"

# Number of blocks
NUM_BLOCKS_GPT2 = 12
